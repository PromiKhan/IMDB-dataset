{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/promiskhan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/promiskhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "x_train = pd.read_csv('x_train.csv',header=None)\n",
    "x_test = pd.read_csv('x_test.csv',header=None)\n",
    "y_train = pd.read_csv('y_train.csv',header=None)\n",
    "y_test = pd.read_csv('y_test.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting column headers\n",
    "x_train.columns=['domain','review']\n",
    "x_test.columns=['domain','review']\n",
    "y_train.columns=['sentiment']\n",
    "y_test.columns=['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnecessary characters from reviews\n",
    "x_train['review']= x_train['review'].str.replace('[^A-Za-z0-9\\s]+', '')\n",
    "x_test['review']= x_test['review'].str.replace('[^A-Za-z0-9\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all the letters to lowercase from reviews\n",
    "x_train['review']= x_train['review'].str.lower()\n",
    "x_test['review']= x_test['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For tokenizing the data on whitespace\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "#For lemmatizing using wordnet\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#Function to tokenize and lemmatize data\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text)])\n",
    "x_train.review = x_train['review'].apply(lemmatize_text)\n",
    "x_test.review = x_test['review'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4313\n"
     ]
    }
   ],
   "source": [
    "#Vocabulary size\n",
    "vect = CountVectorizer()\n",
    "X_train_1 = vect.fit_transform(x_train.review)\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "X_test_1 = vect.transform(x_test.review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - classification and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function for accuracy\n",
    "def print_acc(model):\n",
    "    predicted = model.predict(x_test.review)\n",
    "    target_names = ['positive','negative']\n",
    "    print('Accuracy: %.2f' % (accuracy_score(predicted,y_test.sentiment)*100))\n",
    "    print('-'*30)\n",
    "    print('Confusion matrix: ')\n",
    "    print(confusion_matrix(predicted, y_test.sentiment))\n",
    "    print('-'*30)\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(predicted,y_test.sentiment, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing common words\n",
    "stop_words = list(stopwords.words('english'))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.8221\n",
      "Best parameters:  {'clf__C': 10, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
      "Accuracy: 79.67\n",
      "------------------------------\n",
      "Confusion matrix: \n",
      "[[253  75]\n",
      " [ 47 225]]\n",
      "------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.84      0.77      0.81       328\n",
      "    negative       0.75      0.83      0.79       272\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.80      0.80      0.80       600\n",
      "weighted avg       0.80      0.80      0.80       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pipelining count vectorizer with logistic regression \n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.5)),\n",
    "                     ('clf', LinearSVC())])\n",
    "#Preparing parameters for grid search cross validation\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2,2)],\n",
    "    'vect__stop_words' : [None, stop_words],\n",
    "    'clf__C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "grid = GridSearchCV(text_clf, tuned_parameters, cv=ShuffleSplit(n_splits=10, random_state=123))\n",
    "#Fitting best parameters to build classifier\n",
    "svc = grid.fit(x_train.review, y_train.sentiment)\n",
    "print(\"Best cross-validation score: {:0.4f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "#Performance on test set\n",
    "print_acc(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.8225\n",
      "Best parameters:  {'clf__C': 10, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n",
      "Accuracy: 83.17\n",
      "------------------------------\n",
      "Confusion matrix: \n",
      "[[260  61]\n",
      " [ 40 239]]\n",
      "------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.87      0.81      0.84       321\n",
      "    negative       0.80      0.86      0.83       279\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.83      0.83      0.83       600\n",
      "weighted avg       0.83      0.83      0.83       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pipelining Tfidf vectorizer with logistic regression \n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.5)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LinearSVC())])\n",
    "#Preparing parameters for grid search cross validation\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2,2)],\n",
    "    'vect__stop_words' : [None, stop_words],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "grid = GridSearchCV(text_clf, tuned_parameters, cv=ShuffleSplit(n_splits=5, random_state=123))\n",
    "#Fitting best parameters to build classifier\n",
    "svc = grid.fit(x_train.review, y_train.sentiment)\n",
    "print(\"Best cross-validation score: {:0.4f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "#Performance on test set\n",
    "print_acc(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4201\n"
     ]
    }
   ],
   "source": [
    "#Final Vocabulary size\n",
    "vect = CountVectorizer(max_df=0.5, stop_words=stop_words)\n",
    "X_train_1 = vect.fit_transform(x_train.review)\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "X_test_1 = vect.transform(x_test.review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing prediction for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#Predictions for short simple sentences\n",
    "review = \"This movie is good\"\n",
    "print(svc.predict([review]))\n",
    "review = \"This movie is bad\"\n",
    "print(svc.predict([review]))\n",
    "review = \"This movie is not good\"\n",
    "print(svc.predict([review]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: it only recognizes the phone a it storage device\n",
      "Sentiment:  0\n",
      "Predicted Sentiment:  [1]\n",
      "Review: the one big drawback of the mp3 player is that the button on the phone front cover that let you pause and skip song lock out after a few second\n",
      "Sentiment:  0\n",
      "Predicted Sentiment:  [0]\n",
      "Review: this phone is pretty sturdy and ive never had any large problem with it\n",
      "Sentiment:  1\n",
      "Predicted Sentiment:  [1]\n"
     ]
    }
   ],
   "source": [
    "#Predictions for amazon\n",
    "print(\"Review:\",x_test.review[0])\n",
    "print(\"Sentiment: \",y_test.sentiment[0])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[0]]))\n",
    "print(\"Review:\",x_test.review[2])\n",
    "print(\"Sentiment: \",y_test.sentiment[2])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[2]]))\n",
    "print(\"Review:\",x_test.review[165])\n",
    "print(\"Sentiment: \",y_test.sentiment[165])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[165]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: the result is a film that just dont look right\n",
      "Sentiment:  0\n",
      "Predicted Sentiment:  [0]\n",
      "Review: it came free with a dvd player i bought but i still turned the thing off halfway through because i wa embarrassed for howell\n",
      "Sentiment:  0\n",
      "Predicted Sentiment:  [0]\n",
      "Review: but it wasnt until i watched this film that i realised how great he actually wa\n",
      "Sentiment:  1\n",
      "Predicted Sentiment:  [1]\n"
     ]
    }
   ],
   "source": [
    "#Predictions for imdb\n",
    "print(\"Review:\",x_test.review[205])\n",
    "print(\"Sentiment: \",y_test.sentiment[205])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[205]]))\n",
    "print(\"Review:\",x_test.review[275])\n",
    "print(\"Sentiment: \",y_test.sentiment[275])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[275]]))\n",
    "print(\"Review:\",x_test.review[388])\n",
    "print(\"Sentiment: \",y_test.sentiment[388])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[388]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: kind of hard to mess up a steak but they did\n",
      "Sentiment:  0\n",
      "Predicted Sentiment:  [1]\n",
      "Review: i really do recommend this place you can go wrong with this donut place\n",
      "Sentiment:  1\n",
      "Predicted Sentiment:  [1]\n",
      "Review: everything wa fresh and delicious\n",
      "Sentiment:  1\n",
      "Predicted Sentiment:  [1]\n"
     ]
    }
   ],
   "source": [
    "#Predictions for yelp\n",
    "print(\"Review:\",x_test.review[455])\n",
    "print(\"Sentiment: \",y_test.sentiment[455])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[455]]))\n",
    "print(\"Review:\",x_test.review[505])\n",
    "print(\"Sentiment: \",y_test.sentiment[505])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[505]]))\n",
    "print(\"Review:\",x_test.review[595])\n",
    "print(\"Sentiment: \",y_test.sentiment[595])\n",
    "print(\"Predicted Sentiment: \",svc.predict([x_test.review[595]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
